# Optimization

The Optimization problems of finding the most appropriate ("best") solutions out of all feasible solutions are present every where, in operations research, supply chain, manufacturing, design, games, etc. Often this involves finding the maximum or minimum value of some function under certain constraints: the minimum time to make a certain journey, the minimum cost for doing a task, the maximum power that can be generated by a device, and so on.

These are also among the most difficult problems in computer science and belong to a complexity class for decision problems called NP-Complete Problems. These problems have two properties, we can check a solution quickly(in polynomial time) and if we can solve one NP-complete quickly, we can solve them all.

A well designed optimization model provide unique insights into the situations, compare different scenarios, aid in what-if analysis, revealing areas of possible improvements and so on. Every optimization problem has three components: an objective function, decision variables, and constraints.

Mathematically, optimization problem operate on a vector of variables $x=(x_1, \dots, x_n)$ that are to be optimized to find an optimal solution vector $x^*$ such that the objective function $f_0: \mathbb R^n \rightarrow \mathbb R$ is maximized or minimized, satisfying certain bounds and constraints $f_i(x) : \mathbb R^n \rightarrow \mathbb R, i=1, \dots, m$ and takes the form:

$$
\begin{array}{ll}
\text{minimize} & f_0(x) \\
\text { subject to } & f_i(x) \leq b_i, \quad i=1, \dots, m
\end{array}
$$

The optimal solution $x^*$ has the smallest value of $f_0$ among all vectors that satisfy the constraints.

Many class of optimization problems are characterized by these objective and constraint functions

- Based on type of variables we are optimizing
  - Continuous:
  - Discrete:
    - Combinatorial Optimization: Focuses on discrete choices where solution space is finite. Often involves combinatorial structures like graphs, sets, or permutations. Eg: Traveling Salesman Problem, Minimum Spanning Tree, KnapSack Problem.
    - Integer programming: Can involve infinite discrete sets
- Based on Constraints on the variables
  - Unconstrained: Objective functions and the optimization variables are not constrained or bound.
  - Constrained Optimization:
    - Sparse: if each constraint function depends on only a small number of variables
- Based on Linearity of Objective and constraint functions
  - Linear Programming (LP): if the objective and constraint functions $f_0 , . . . , f_m$ are linear (i.e $f_i (\alpha x + \beta y) = \alpha f_i (x) + \beta f_i (y)$)
    - Integer Linear Programming (ILP)
    - Mixed Integer Programming (MIP)
  - Non-Linear Programming (NLP): If objective or constraints are non-linear.
    - Local optimization methods: find a point that minimizes the objective function among feasible points near it. Fast can handle large problems. Requires initial guess. Provide no information about distance to (global) optimum.
    - Global optimization methods: find the (global) solution. Worst-case complexity grows exponentially with the size of the problem.
- Based on Convexity of the objective or constraints functions (more general than linearity)
  - Convex Optimization :the objective and constraint functions are convex, it must satisfy
    $$f_i (\alpha x + \beta y) \leq \alpha f_i (x) + \beta f_i (y)), \forall x, y \in \mathbb R^n \text{ and } \forall \alpha, \beta \in \mathbb R : \alpha + \beta = 1, \alpha \geq 0, \beta \geq 0$$
  - Non-Convex Optimization
- Based on the number of objective functions
  - None
  - One
  - Many
- Based on Functional Smoothness
  - Differentiable
  - Non-Differentiable
- Deterministic vs Stochastic
  - Deterministic algorithms have specific rules for moving from one solution to another. Data for problem is known accurately
  - Stochastic Algorithms have probabilistic transition rules. Data cannot be known precisely for various reasons (measurement error, future uncertainty).

 General Optimization problems are very difficult to solve. Most methods involve some form of compromise like taking very long time to compute or finding an approximate solution. Certain classes of problems can be solved efficiently and reliably.

- Linear Programming (LP) such as simplex algorithm are one of the basic techniques with tremendous capabilities.
- Quadratic Programming (QP) problems: objective function being linear or quadratic function of the decision variable

- Convex Optimization Problems
- Local Search (LS)
- Evolutionary Algorithms

For most AI uses cases, we use the following optimization

Discrete optimization: find the best discrete object.
$$\min_{p \in \text{Paths}} Cost(p)$$
Algorithmic tool : Dynamic Programming

Continuous Optimization: find the best vector of real numbers
$$\min_{w \in \mathbb R^d} \text{TrainingError}(w)$$
Algorithmic tool: gradient descent.

## Applications

- Portfolio Optimization: optimal allocation of capital investment in a set of n assets, maximizing overall return or minimizing the overall risk(variance), under limited non-negative budget and minimum acceptable expectable return on the whole portfolio.
- Design(Mechanical, Civil, Electronic, ...): finding optimal dimensions for the component under variety of engineering requirements
- Machine Learning/Data Fitting: Find optimal model parameters that optimizes the loss functions (misfit or prediction error) constrained by prior information or limits on the parameters.
- Automatic Control
- Supply Chain Problems
  - Routing
  - Scheduling

## History of Optimization

- Calculus of Variations & Early Optimization (1600s–1800s)
  - Fermat: early ideas of maxima/minima.
  - Euler–Lagrange equations: optimizing functionals, such as the shortest curve or minimal action.
  - Lagrange multipliers (Joseph-Louis Lagrange, 1797):
  - Foundation for constrained optimization.

- Classical Linear & Nonlinear Optimization (1900–1960)
  - Linear Programming (LP) formalized.
  - Simplex method by George Dantzig (1947): revolutionized operations research.
  - Nonlinear programming methods developed for engineering design.

- KKT Conditions & Modern Nonlinear Optimization (1950–1980)
  - Karush–Kuhn–Tucker (KKT) conditions for nonlinear constrained optimization.
  - Quadratic programming, convex duality, interior-point methods appear.
   1960s: early interior-point methods (Fiacco & McCormick, Dikin, . . . )
  - 1970s: ellipsoid method and other sub-gradient methods

- Convex Optimization Revolution (1980–2000)
  - 1980s: polynomial-time interior-point methods for linear programming (Karmarkar 1984)
  - Nesterov & Nemirovski (1994): polynomial-time interior point methods for non-linear convex problems.
  - Rockafellar: convex analysis becomes central.
  - before 1990: mostly in operations research; few in engineering, later got new applications in engineering (control, signal processing, communications, circuit design, ...);

   Convex optimization becomes a dominant mathematical discipline.

- Machine Learning Era (2000–present)
  - Gradient-based optimization becomes ubiquitous (SGD, Adam, RMSProp).
  - Non-convex optimization central to deep learning.
  - Distributed optimization: federated learning, giant-scale training (LLMs).
   Optimization has become *the engine of modern AI*.

## Julia Ecosystem for Optimization

- [Jump](https://github.com/jump-dev/JuMP.jl) : An algebraic modeling language for linear, quadratic, and nonlinear constrained optimization problems.
- [JuliaOptPackages](http://www.juliaopt.org/packages/)
  - [Convex.jl](https://github.com/JuliaOpt/Convex.jl): An algebraic modeling language for disciplined convex programming.
- [JuliaSmoothOptimizers](https://github.com/JuliaSmoothOptimizers) provides a collection of tools primarily designed for developing solvers for smooth nonlinear optimization.
- [JuliaNLSolvers](https://github.com/JuliaNLSolvers) offers implementations in Julia of standard standard optimization algorithms for unconstrained or box-constrained problems such as BFGS, Nelder-Mead, conjugate gradient, etc.

## References

- [Convex Optimization by Stephan Boyd, Lieven Vandenberghe](https://web.stanford.edu/~boyd/cvxbook/)
- [Optimization Models by Calafiore and El Ghaoui]
- [Julia for Operation Research](https://juliabook.chkwon.net/book) by Changhyun Kwon
- [Julia Opt](https://github.com/JuliaOpt/juliaopt-notebooks)
